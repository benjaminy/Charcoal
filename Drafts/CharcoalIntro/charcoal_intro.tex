\documentclass[10pt,preprint]{sigplanconf}

% The following \documentclass options may be useful:

% preprint      Remove this option only once the paper is in final form.
% 10pt          To set in 10-point type instead of 9-point.
% 11pt          To set in 11-point type instead of 9-point.
% authoryear    To obtain author/year citation style instead of numeric.

\usepackage{graphicx}

\begin{document}

\newcommand{\charcoal}{Charcoal}

\special{papersize=8.5in,11in}
\setlength{\pdfpageheight}{\paperheight}
\setlength{\pdfpagewidth}{\paperwidth}

\conferenceinfo{CONF 'yy}{Month d--d, 20yy, City, ST, Country}
\copyrightyear{2014}
\copyrightdata{978-1-nnnn-nnnn-n/yy/mm}
\doi{nnnnnnn.nnnnnnn}

\titlebanner{Preprint.  Please do not redistribute}        % These are ignored unless
\preprintfooter{Preprint.  Please do not redistribute}   % 'preprint' option specified.

\title{Pseudo-Preemptive Threads: A Framework for Writing Reliable, Composable and Maintainable Multitasking Software}
\subtitle{Threads and Events Are a Both Bad Ideas}

\authorinfo{Benjamin Ylvisaker}
           {Affiliation1}
           {Email1}
\authorinfo{Name2\and Name3}
           {Affiliation2/3}
           {Email2/3}

\maketitle

\begin{abstract}
This is the text of the abstract.
\end{abstract}

\category{CR-number}{subcategory}{third-level}

% general terms are not compulsory anymore,
% you may leave them out
\terms
term1, term2

\keywords
keyword1, keyword2

\section{Introduction}

It is hard to write multitasking software that is reliable, composable and maintainable.
The most popular abstractions for building such software (events, threads, coroutines) all have significant weaknesses.

This paper describes a new abstraction called \emph{pseudo-preemptive threads} (or \emph{activities}) that promises to make writing multitasking software easier.
To validate the ideas proposed in this paper we implemented activities in a dialect of C called \charcoal.
We wrote several microbenchmarks to explore the performance implications of activities.
We also tweaked two real-world multithreaded C programs to use activities instead of threads to explore the software engineering implications.

This paper starts with brief a description and critique of the dominant multitasking abstractions.

Next we describe activities and provide abstract arguments for why activities do not suffer from the reliability, composability and maintainability weakness of other abstractions.

Next the paper describes the high points of the \charcoal{} implementation.
All the code for the implementation and tests described in this paper is available in a public repository on GitHub.

Finally, we report on the results of our microbenchmarks and real-world application implementations.

\section{Established Multitasking Abstractions}

The three most widely used abstractions for multitasking today are events, threads and coroutines.
Each suffers from serious software engineering problems that we categorize into reliability, composability and/or maintainability.
In this section we describe these problems; in later sections we argue that activities completely avoid or substantially mitigate all of them.

\subsection{Events}

Events are the most popular abstraction for relatively simple multitasking patterns.
Most GUI frameworks use events for things like mouse clicks and keyboard presses.
Until recently this was the only available multitasking abstraction in some popular ecosystems, like browser JavaScript.

When an event dispatcher calls a handler/listener procedure, that call must return before another event can be handled.
This makes avoiding many kinds of concurrency bugs easy.
However, it comes at a high cost in maintainability and reliability for complex applications.

Potentially long-running (or blocking) tasks must be manually broken up into smaller handler procedures by application programmers.
This leads to a style of programming referred to as \emph{stack ripping}\cite{Adya2002}, or more colloquially \emph{callback hell}.
Callback hell can make it hard to decipher the logical flow of a single task through multiple callbacks.
This is bad for reliability, because it is easy to make a mistake in the lifetime of some piece of data (compared with, for example, threads) \cite{Behren2003a}.
Callback hell is even worse for maintainability, because choices about how to break an application up into handlers can be hard to reconstruct down the road.

\subsection{Threads (Preemptive)}

Events are at the safest and least flexible end of the multitasking abstraction spectrum and threads occupy the opposite extreme.
(In this paper \emph{thread} means \emph{preemptive thread}.)

The primary strength of threads is that they can wait/block indefinitely and/or run for a very long time without preventing other threads from making progress.
This makes it possible to write multitasking software in a natural single-task style (i.e. threads completely avoid callback hell).

The primary weakness of threads is that they make it far too easy to introduce concurrency bugs like data races, deadlocks, atomicity violations and livelocks.
In the last decade a significant amount of research effort has been devoted to making it easier to write reliable multithreaded applications, because of the connection to multicore processors.
While some of this research is quite impressive, most mainstream application programmers still view threads as too dangerous for multitasking programming (correctly in the current authors' opinion).

Applications that use threads and conventional concurrency control mechanisms also suffer from serious composability issues \cite{Harris2005, Grossman2007}.
The very brief summary of the arguments from the cited papers is that when using locks and semaphores, application programmers must design and (somehow) enforce subtle global concurrency control properties.
Using transactions instead of conventional concurrency control mechanisms promises to make threads much safer and more composable.
Unfortunately, it is not clear that it is possible to build a practical implementation of transactional memory that is compatible with mainstream programming practices.

Among the multitasking abstractions discussed in this paper, threads are unique in that they permit the parallel execution of tasks.
From the perspective of multitasking, parallelism is largely irrelevant; multitasking abstractions like events and coroutines can be used in tandem with parallelism abstractions like threads and processes.
Nevertheless, in the discussion section at the end of this paper we speculate on the feasibility of running activities in parallel.

\subsection{Coroutines}

Coroutines have an interesting history.
According to Knuth, the term was coined by Melvin Conway in 1958, but coroutines remained on the margins of mainstream software practice until recently.
For example, the async/await framework (coroutines by a different name) was added to C\# in version 5, which was released in 2012, \texttt{function*} (coroutines by a different name) was added to the 2015 revision of ECMAScript, and D4134 is a proposal to add coroutines to C++17.
We consider the adoption of coroutines in these extremely widely used ecosystems to be evidence that mainstream software developers have been writing increasingly sophisticated multitasking patterns in recent years, making life in callback hell even more painful.

Using the coroutine abstraction requires application programmers to partition procedures into normal procedures (functions, methods, subroutines, whatever) and coroutines.
In most implementations the procedure calling syntax is overloaded; what appears to be a call to coroutine is actually a task spawn.
Within the body of a coroutine definition, a \emph{yield} (or \emph{await}) primitive can be used to permit switching execution flow to a different task (i.e. coroutine).
Invoking yield in a normal procedure (a non-coroutine) is not permitted.

Relative to event loops, coroutines provide greater flexibility, because multiple tasks can be in progress at the same time.
Coroutines are much more resistant to concurrency bugs than threads, because only one coroutine can be active at a time, and the application explicitly states when switching between tasks is permitted.

The primary weakness of coroutines is a subtle tension with conventional procedural abstraction.
It is common for application programmers to want to yield in a procedure called by a coroutine, but this is not possible.
This can be quite inconvenient on its own, and it makes refactoring strategies like procedure extraction trickier to apply.
Another consequence of this issue is that coroutines tend to be viral; if a programmer decides to convert a procedure to a coroutine (for example because it needs to wait for the arrival of a network message), it tends to be the case that any callers of that procedure need to be converted to coroutines as well.

Similarly, higher-order function patterns get more complicated as well.
There are now two distinct kinds of procedure-like-things [clarify/expand].

These software engineering awkwardnesses have not prevented the adoption of coroutines, but they are annoying.
Activities solve these problems!

\subsection{Cooperative Threads}

Cooperative threads can be seen as a compromise between coroutines and threads.
Like coroutines, cooperative threads must invoke a yield primitive to switch from one task to another.
Like threads, cooperative threads do not have a separate kind of procedure (i.e. coroutine/async) and yield can be invoked anywhere (i.e. not restricted to coroutine bodies).

Cooperative threads have a tension with procedural abstraction that is complementary to the tension in coroutine abstractions.
The atomicity properties of a particular procedure depend on whether or not any of the procedures it calls will invoke yield.
This can be annoying when initially writing code, and it is especially problematic during maintenance.
If yield is added to a procedure that did not previously have it, there is the possibility that any caller of that procedure will have its atomicity properties violated.
This is extra nasty when indirect calls are considered, because it is not possible in general to identify all call sites to a particular procedure.

A quick note on terminology: It is fairly common to see the term \emph{coroutine} applied to what we call \emph{cooperative threads}.
In the context of such usage, what we call coroutines are often called \emph{stackless coroutines}.
Naturally, we prefer the terms as defined in this paper.

\subsection{Others}

There are other approaches to multitasking that we briefly mention here only to argue that they are not directly relevant to the main points of this paper.

Isolated processes can be used for multitasking, but are far too heavy for the kinds of applications we are focused on (like GUI events and asynchronous network downloads).

Functional reactive programming (FRP) is an entirely different approach to multitasking.
It is interesting, but still at an early enough stage of research that it is not clear whether it can be integrated with mainstream programming practice.

\section{Pseudo-Preemptive Threads}

Before defining pseudo-preemptive threads, we start with an exampel stolen from \cite{Krohn2007}.
This example function concurrently fetches DNS information using the standard \texttt{getaddrinfo} procedure.

\hspace{-0.5cm}
\includegraphics{multi_getaddrinfo.pdf}

There are things worth noting about this code.
The central syntactic addition is the \emph{activate} statement (line 10).
The body of an activate statement executes concurrently (but not in parallel) with whatever comes after the statement (i.e. its continuation).
The program can only switch between activities on a yield execution, but there are many implicit yields.
Any blocking or potentially long-running system call is rewritten to start, then yield.
Also every loop has an implicit (in the source code) yield at the end of every iteration.

These automatically-inserted yields make activities behave more like threads than events or coroutines from an application programmer's perspective.
However, there is a big difference between activities and threads.
Notice that multiple activities read and write the local variable \texttt{requested}.
If these were threads, this would cause a data race, which would make the whole program undefined under the currently dominant memory models.
With activities this is not a problem at all; the system (compiler, processors, etc) is not permitted to move memory operations across yields, which makes the concurrent memory model problem go away.

The activate statement takes as a parameter a pointer to application-allocated memory for storing necessary metadata about the new activity.
In the above example the memory for storing activity information is allocated locally.
Because of this the example procedure must bock waiting for all the activities to finish.
(Returning before they finish would cause those activities to access deallocated memory).
It would be possible to change the interface so that the caller would have to pass in the backing memory.
In that case, the procedure could return while the activities were still fetching DNS information, allowing the application to go on with other work.

Note that as compact as this example is, it incorporates a slightly fancy feature of limiting the number of concurrent network connections to an application-specified number.

The parentheses after the activate keyword are for controlling whether local variables are access by-value or by-reference.
The default is by-reference, which is how all the variables are used in this example.
In some cases it is more convenient to capture the value of a local variable at activation time.
This can be done by simply listing the variable inside the parentheses.
Making a local variable by-value effectively makes two copies of it that can be updated independently.

\subsection{Don't know}

Additionally, like all properly implemented cooperative threads, activities cannot suffer from data races

\cite{Boehm2011}


\subsection{}

In this section we describe the new abstraction we call \emph{pseudo-preemptive threads}, or \emph{activities}.
Activities can be seen as a compromise between cooperative threads and preemptive threads.
Like cooperative threads, only one activity can be running at a time and switching between activities can only happen when yield is invoked.
However, in order for a language to support activities it must implicitly and frequently invoke yield.
(What we mean by frequently is explained in detail below.)
This frequent yielding makes the behavior of activities from an application programmer's perspective more like preemptive threads.

If there was no way to limit yielding, activities would suffer from many of the same concurrency bug challenges as threads.
To combat this we add no-yield, a property that can be added to statements and procedures.
No-yield is syntactically similar to \texttt{synchronized} in Java.
In the dynamic scope of a no-yield block (statement or procedure), the current activity cannot be interrupted.
This is a simple and powerful tool for enforcing atomicity.
We expect that most of the uses of locks in current multithreading practice can be directly replaced by the simpler and safer no-yield.

Because activities are like threads in that tasks can be written in a natural (not callback hell) style, activities do not suffer from the maintenance problems of events.
Because the primary concurrency control mechanism (no-yield) enforces a simple global property, activities do not suffer from the composability problems of threads.
Because the concurrency control is generally simple, activities do not suffer from the reliability problems of threads.
To be fair, applications written with activities will surely have concurrency bugs; it seems impossible to write multitasking software with zero exposure to concurrency bugs.
However, ...

\subsection{Yield Frequency}

Part of the definition of activities is that by default yields should happen ``frequently''.
But what does this mean precisely?
The answer depends on the details of the rest of the programming language definition.
However we can state two design rules that apply to any language.
These rules are in strong tension with each other:

\begin{enumerate}
\item It should \textbf{not} be possible for an activity to run indefinitely without executing a yield.
\item Yields should happen as infrequently as possible.
\end{enumerate}

One clear consequence of these rules is that anything that could cause an activity to pause indefinitely (e.g. a syscall) by default must be modified to allow other activities to run while the paused activity waits.
More challenging, loops must be interrupted.

\subsubsection{Recursive Procedure Calls}

The current design of Charcoal does not follow rule \#1 perfectly.
Function calls and returns do not implicitly yield, which means that recursion can be used to make an activity run indefinitely without yielding.
We consider this a bug, not a feature, in the language design, but we have not found any less bad alternatives.

The most obvious approach to avoiding yield-free recursion is to say that every call and/or return has an implicit yield.
This idea is bad for two reasons.
The simpler reason is that it violates rule \#2; calls and returns happen all the time and introducing a yield for every call would be very costly for performance.
The more important reason has to do with procedural abstraction.
If calls carried implicit yields, then the function extraction refactoring pattern would change the concurrency behavior of the program.
This seems totally unacceptable.

One could imagine trying to identify recursive calls specifically and saying that only recursive calls carry an implicit yield.
However, with indirect calls it is impossible to precisely statically analyze which calls are recursive in general.
This means that the language design would have to codify some rules about which classes of calls could be guaranteed to be analyzed as recursive, which seems like a fragile design.

\subsubsection{Programmer Control}

It would be bad if programmers could not control activity yielding in some way.
In Charcoal there are two primary tools for controlling yielding: no-yield and explicit yielding.
Any procedure or statement can be marked ``\texttt{no\_yield}'' which means that the activity will not yield during the execution of the statement or call to the procedure.

\section{Implementation}

There are a few interesting features of our implementation of activities.
The the following sections we describe the allocation of call frames, yielding and no-yield versions, and yield implementation.

\subsection{Call Frame Allocation}

One of the tricky issues in the implementation of any thread-like framework is the allocation of procedure call frames.
The first few sections below describe existing strategies for frame allocation in multithreaded code.
Then we describe a new approach that we call locally contiguous allocation.

\subsubsection{Conventional Contiguous Allocation}

In single-thread applications it is convenient and efficient to allocate a single large region of memory for \emph{the} stack of call frames.
Individual frames are allocated contiguously in this space.
This works well because the live ranges of frames are strictly nested; a callee's frame is deallocated before its caller's.
In multithreaded applications this strategy does not work, because each thread needs to be able to allocate and deallocate frames independently.

The most common strategy for multithreaded frame allocation is to pre-allocate a moderately large range of memory in the heap for each thread.
With this approach individual frame allocation can be done exactly as in the single-thread case.
This is fast and simple, but is not very memory efficient.
To avoid stack overflows, the application must ensure that the allocated blocks are large enough for the deepest chain of procedure calls.
Most existing software takes a very conservative approach to this, leading to significant chunks of memory going unused.

Furthermore, the large memory block per thread approach interacts in an unfortuante way with common virtual memory strategies.
Threads that have only a couple of frames on their stack generally still need a whole page in physical memory, leading to a significant chunk of physical memory going unused.

\subsubsection{Individual Heap Allocation}

A completely different approach to call frame allocation is to individually allocate each frame in the heap.
This eliminates the need to pre-allocate a large chunk of memory at thread creation time and the granularity issue mentioned above.
However, heap allocation of frames comes at a significant performance cost for call and return operations.

Simple implementations of heap allocation are generally more than an order of magnitude slower than contiguous allocation (see more details in the microbenchmarking section below).
If engineered carefully, heap allocation can be much faster than that (e.g. \cite{Shao2000}).
However, even the most efficient implementations that we are aware of are still substantially slower than contiguous allocation for call-heavy code.

\subsubsection{Split/Segmented Stacks}

For completeness, we briefly mention another strategy called \emph{split} or \emph{segmented} stacks.
The idea is that stack space is allocated in small chunks (or segments).
The common case call/return execution looks like traditional contiguous allocation.
When a thread reaches the end of its segment it allocates a new one and links them together.
This idea has a lot of appeal (the implementers of Rust and Go both used it), but it has some really unpleasant uncommon case behavior (the implementers of Rust and Go both
abandoned it in later versions).

\subsubsection{Locally Contiguous Call Frame Allocation}

The new call frame allocation strategy that we implemented is a hybridization of contiguous and individual help allocation.
The idea is based on the observation that the slowness of heap allocation only matters for short-lived calls.
For long-lived calls (i.e. calls nearer the base of the call stack), the overhead of the call and return operations can be amortized over the long running time of the call and its callees.

How can the implementation know which calls will be short-lived and which long-lived?
In a language with activities, the programmer explicitly says that with no-yield annotations!
With activities it is good style to mark short-running procedures as no-yield to avoid the possibility of context switching in the middle of executing them.
This is good for avoiding concurrency bugs and it interacts perfectly with our frame allocation strategy.
Only one activitiy can be running in no-yield mode at a time, so an application only needs a single large region of memory for contiguous frame allocation.
All activities can share this single chunk of memory.

So our implementation of \charcoal uses individual heap allocation for calls made in yielding mode and contiguous allocation in a single shared area of memory in no-yield mode.
In the microbenchmarking section below we provide evidence that this strategy captures most of the benefits of heap allocation (no large per-activity memory overhead) and contiguous allocation (fast calls and returns when it matters most).

\subsection{Fast and Slow}

One of the important features of the implementation is that there are
two versions of each procedure: an unyielding version and a might-yield
version.

Wait, don't we know whether a procedure is yielding or not?  Well, yes,
but \ldots For procedures marked unyielding, we know there is no need to
compile a might-yield version.  For other procedures we generally need
to compile both versions, because the procedure might be called in
either of the contexts.  The compiler might be able to do some analysis
to prove that one or the other version is never called and can therefore
be excised as dead code.

This is kinda related to ideas from the Cilk-5 implementation
\cite{Frigo1998}.

\subsection{Yield Implementation}

\section{Benchmarking}

To establish the practicality of activities, we implemented N
microbenchmarks to compare our implementation of Charcoal against C with
threads and/or libuv (a popular event loop library written in C).

The first couple microbenchmarks demonstrate that basic concurrency primitives, like task spawning and task switching, are far faster with activities than threads.
This is not particularly surprising.
Because threads can be interrupted at any time, context switching requires the operating system to copy all processor state to memory.
Thread creation requires the allocation of lots of system resources.

\subsection{Task Limit}

The first microbenchmark measures how many concurrent tasks can exist.
To measure this the benchmark simply spawns tasks until the program crashes; however many tasks existed before the crash is the limit.
This benchmark really measures the memory overhead of a task.
For activities and event listeners this overhead is small and more or less fixed.
For threads this overhead is harder to quantify, because most multithreading APIs allow the amount of memory reserved for the call stack to be controlled by the application.
For this benchmark we simply used the default stack size in the installed pthreads implementation.
The results of this test are not surprising, but we believe it is worth emphasizing that it is not practical to use more than a few hundred or maybe a few thousand threads, whereas the limit on activities or event listeners is several orders of magnitude higher.

\subsection{Task Spawn Speed}

% wait for previous task
% spawn new task

The second microbenchmark measures how quickly new tasks can be spawned.
The task in this test waits for the previous task to finish, then spawns the next task.
With pthreads, our test system was able to spawn about 60 threads per millisecond; our activities implementation was about to spawn over 2,000 per millisecond.
This difference of well over an order of magnitude is significant, because it means that it can be efficient to spawn an activity for much smaller units of work than it would be to spawn a thread.
In other words, there is no reason to build \emph{activity pools}; it is always a better idea to simply spawn an activity for the task at hand.

\subsection{Task Switching}

The third microbenchmark measures how quickly the system can switch from one task to another.
For this test we spawned 20 tasks and organized them in a ring, each waiting for a signal of some kind from its neighbor.
The test injects a signal at one point and then measures how quickly it can cycle around the ring.
On our system we measured about 600 switches per millisecond with pthreads and 5,000 per millisecond with activities.
This difference is almost an order of magnitude.
Like the spawning benchmark, the main takeaway is that the overhead for activities is low enough that individual activities can do quite small units of work per context switch without paying a high overall efficiency penalty.

\subsection{Just Calling}

% int f( int d, int x )
%     if( d > 0 )
%         return f( d - 1, f( d - 1, x ) )
%     else
%         return small_computation( x )

The fourth microbenchmark measures the overhead of heap-allocating call frames.
This test has a very simple recursive function that calls itself twice.
At the leaves of the recursion is a very small computation, just complicated enough to prevent a smart compiler from just statically computing the whole answer.
For this benchmark plain C is more than an order of magnitude faster than Charcoal.
This difference is painful, but there are several things to say about it.
First, the Charcoal implementation is a research prototype, so it is quite likely that good engineering work would close the gap to some extent.
Second, this is a microbenchmark; no application spends all of its time in calls and returns, so real application performance impact will be proportionally smaller.
Third, procedure calling in no-yield mode is just as efficient as plain C, which means that the programmer has some control over this overhead.

To investigate this foo further, ...

\subsection{Yielding}

The yield primitive is implemented to be very fast when the current activity will continue executing (i.e. not switch to another activity).
The key implementation trick here is described in section \ref{sec:blah}.
Even with this clever implementation, yielding is still not free.
To quantify its expense, we benchmarked a simple but problematic function: \texttt{strcmp}.
\texttt{strcmp} is tricky for a few reasons:
First, the strings passed in might be very long, so it is not acceptable to make the implementation simply never yield.
Second, the body of the loop is extremely simple; good implementations are just a few assembly instructions.
This means that yielding every iteration would be extremely painful.
Third, whether each iteration executes depends on the computation of the previous iteration, so simple loop tiling/blocking tricks don't work.
The best-performing implementation we have found so far looks like:

%    while_no_yield( *s1 )
%        if( *s1 - *s2 )
%            break;
%        ++s1; ++s2;
%        if( !( s1 & B ) )
%            yield

\texttt{while\_no\_yield} is a special variant of the regular while loop that does not have an implicit yield after every iteration.
This is different from wrapping a regular while loop in \texttt{no\_yield}, because the latter would prevent nested yields from happening, whereas the former does not.

In the code about \texttt{B} is a bit-mask with some number of the low bits set to one.
The effect is that once every $2^N$ iterations there is a yield, where $N$ is the number of bits set in the mask.
This implementation gets the overhead down to 15\% (???), which is neither awful nor good enough to never think about this issue again.
One reasonable workaround is that when callers of \texttt{strcmp} are certain that the strings are not especially long, they can make the call in no-yield mode.
In this case, it should be just as efficient as plain C.

memory limit: thread: 1,000 activity: 1,000,000 libuv: 1,000,000,000

spawns per ms: thread: 60 activity: 2,400 libuv: 7,100

context switching (bucket brigade): thread: 630  activity: 4,900

just calling: C: 590  Charcoal: 13

strcmp: C: 1000 Charcoal: 100



\section{Foreign Code}

Foreign code (including legacy code) will never yield.  This could lead
to starvation pretty easily.  Here are three strategies:

\begin{itemize}
\item Do nothing.
  Just run the foreign code.
  This is a perfectly reasonable strategy as long as the foreign code does not run for a long time.
\item Run the foreign code in its own thread.
  If it has not returned by the end of some time slice, pause it to allow other activities to run.
  This runs the risk of creating atomicity violations galore.
  It also reintroduces the possibility of data races.
  However, it might be a reasonable strategy in situations where there is very little sharing between the foreign code and the rest of the application.
\item Run the foreign code in its own thread, but only interrupt it at special ``safe-ish'' points, like system calls.
  This is a compromise between the previous two strategies in the sense that it opens the door to both starvation and atomicity violations, but provides some (imperfect) protection against both.
\end{itemize}

We have not thought at all about what the best default is or what syntactic sugar would be nice.

Another important implementation issue to consider is foreign code that calls back in to activity-aware code.
There will definitely be some fancy footwork necessary there, no matter which strategy is used.

\acks

Acknowledgments, if needed.

% We recommend abbrvnat bibliography style.

\bibliographystyle{abbrvnat}

% The bibliography should be embedded for final submission.

\begin{thebibliography}{}
\softraggedright

\bibitem[Smith et~al.(2009)Smith, Jones]{smith02}
P. Q. Smith, and X. Y. Jones. ...reference text...

\end{thebibliography}

\end{document}
