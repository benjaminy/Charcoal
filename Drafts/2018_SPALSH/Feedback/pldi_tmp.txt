

* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *


Review #158C
===========================================================================

Overall merit
-------------
B. I support accepting this paper but will not champion it.

Reviewer expertise
------------------
X. Expert

Paper summary
-------------
Compared to preemptive multithreading, cooperative concurrency is appealing for introducing less nondeterminism: fewer distinct interleavings of threads are possible.  However, typically the author of a function must decide whether it returns immediately or yields, and often that choice doesn't match what the author of calling code wants.  This paper proposes an atomic-block primitive for cooperative concurrency, where, effectively, within an atomic block, yields are no-ops, so function authors can be more liberal in including them in the first place.  Based on this idea, a JavaScript library and a C extension are presented.  They both bring in a new notion of activities to help drive the right decisions about grouping continuations into trees of execution.

Comments for author
-------------------
The central idea here is thought-provoking, and the paper presents it well.  If the paper's key hypothesis are right, then it could have quite a lot of practical impact.  In a sense, the paper itself is already prepared for that scenario, being written in a way that should be accessible and appealing to working programmers.  It's an unusual kind of PLDI submission, and I lean toward supporting acceptance.  I can't quite justify championing the paper, though, because of doubts about the key motivating hypothesis.

This paper is written more with the kind of voice and conventions associated with developer-facing blogs than with PLDI.  I feel like it's good for PLDI to pursue diversity on this dimension, in its program, so, all else being equal, it's a plus for a paper to be written this way.  I found the writing very clear, and I enjoyed following along smoothly as the story developed.

The empirical study of JavaScript programs at the start is a contribution in its own right.  It demonstrates that many programs implement linear execution sequences via nested callbacks, with very short delays between registering a callback and having it called.  It makes sense that testing would rarely exercise the case where some other task gets scheduled during these short periods, so that developers might not realize related bugs in their code.  However, as the paper admits, no particular evidence is presented that these bugs show up frequently in practice.

The whole motivation for the new language-design ideas is avoiding bugs, so it's a bit of a problem not to be on firmer footing about how troublesome these bugs are.  (The same ideas also happen to be useful for performance optimization, as the experiments [partly] show, but we could imagine applying related optimizations with no use of activities or other source-level modifications.)  Clearly programs get more complicated textually, and they might even become harder to think about, which is counterintuitive for a change billed as avoiding bugs.  In my own experiences with web programming, I'm not used to worrying about or being bitten by this kind of problem.  I don't think I've just been "getting lucky," as I believe the programs I write are naturally prepared for all interleavings -- it's just straightforward in the scenarios I'm used to.

Cooperative concurrency is appealing for its source-level simplicity, and the ideas presented here lower that simplicity moderately.  Maybe a real worry is being addressed, but it is not clear.  Still, I did enjoy reading the paper, and I don't think it's a bad thing to promote this kind of experimentation.

It may be worth pointing out explicitly that, since the study reported in Section 2 uses a browser (Chromium), you are only analyzing client-side JavaScript, not server-side, which could turn out to have quite different characteristics.

More by line number:

* 404: "depending" should have "on" after it.
* 481: "mutlithreading context"
* 711: "performs in Figure 9 performs"
* 824: "a atomic" should use "an".
* 864: "the implementation the" is missing "of".

Questions for Authors
---------------------
How do you argue that adding atomic blocks to existing concurrent programs won't introduce new bugs via deadlock, where the code in an atomic block could be blocked waiting for some event that only another "thread" can trigger?  And will programmers tend to trip over this danger in writing code from scratch?


* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *


Review #158D
===========================================================================

Overall merit
-------------
D. I will argue against accepting this paper.

Reviewer expertise
------------------
Y. Knowledgeable

Paper summary
-------------
This paper proposes an extension/modification to the cooperative concurrency model for JavaScript’s async functions that (1) implicitly inserts additional yield points around every call, but (2) counters that by also providing atomic blocks in which no yields to the event loop are performed. This model has been prototyped to some extend as a JavaScript library (with limitations) and implemented on a lower level in C.

Comments for author
-------------------
I think this paper has issues on several levels.

First of all, the motivation seems thin. The paper claims repeatedly that there are practical problems with interleavings in cooperative multi-tasking in JavaScript, but fails to include a single example. It merely points to one paper [15]. Similarly, existing alternatives are only briefly mentioned, claimed to have shortcomings, but no details are provided to back that up. That isn’t enough, the authors need to describe their motivation better and provide a sufficiently self-contained story if they want to convince the reader.

The paper proposes a programming model that — in the opinion of this reviewer — undermines the async programming model as originally intended, which is (among other things), based on the property that all execution is atomic except for explicit yield points. The idea in this paper is kind of backwards when you think about it: first take away control from the programmer by inserting implicit yields where the programmer did not write them, then give control back by requiring the programmer to insert atomic blocks to undo that. The authors believe that implicit yield points “create a more predictable environment for programmers”; I find that very hard to believe, and would rather posit the opposite.

A major part of the stated motivation is to allow programmers to force synchronous execution on library functions that provide an async interface. It is true that the choice between sync and async sometimes is a tough one for library designers, and that the right choice may depend on concrete arguments. But that problem can be solved with adequate library design (that e.g. provides options) instead of doctoring the symptoms by allowing client code to retroactively “fix” and override the choices a library author made (presumably for a reason), without the ability to predict the consequences.

In terms of implementation, the paper is somewhat schizophrenic. While the motivation is cooperative multi-tasking in JavaScript, it only provides a library implementation with severe limitations (e.g., it won’t work correctly in conjunction with legacy code or external APIs). Then it gives a “real” implementation in a runtime system, but doesn’t do so for JavaScript but switch to C! Which is a funny choice, given that C does not have async functions or cooperative multi-tasking as a language feature, so there is an obvious impedance mismatch. It’s unclear whether results from such an implementation would be applicable to a JavaScript VM. Why did the authors not implement their ideas in one of the many open source JS VMs?

For efficient implementation, the authors suggest compiling two variants of each function to deal with atomic vs non-atomic invocations efficiently. They suggest that a linker could remove unused copies. This idea would put a lot of burden on the ecosystem for a comparably small feature. (And in fact, the JavaScript ecosystem does not even have a native notion of linking.) The non-atomic version will be expensive either way, since it induces two additional atomic reads and branches for every call.

The paper suggests “hot stacking” as a new implementation technique for implementing segmented stacks, where a contiguous stack is used for all calls inside an atomic block, but other calls get (more expensive) individual heap-allocated stack frames. Unfortunately, this approach has the side effect of inviting programmers to overuse a semantic feature (atomic blocks) to fix performance problems in their code, in a way that may actually break a program in subtle ways.

As a final criticism, the semantics of the atomic construct for JavaScript is never properly defined. The paper merely explains it indirectly in terms of the two implementation approaches it describes, but one is incomplete, the other is for C.

Questions for Authors
---------------------
L441: This program isn’t valid JavaScript. Did you mean “await” instead of “yield”?

L510: Can you expand on this?

L531: Can you expand on this?

L674: It’s not just legacy code, it’s also legacy API, such as the DOM.

L800: Implicit insertion of yield points around calls subtly breaks refactorings like inlining (technically, you lose beta equivalence).

L813: What does before/after a call mean in terms of C’s notion of sequence points?

L911: Please provide a reference instead of a search term.

L952: You are suggesting to fix performance by changing the meaning of the program. Isn’t that inviting abuse?

L1309: “In this paper we presented evidence..." — Actually, that evidence is not included in the paper.
